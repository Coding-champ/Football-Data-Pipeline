name: Enhanced Football Data Pipeline with Database

on:
  schedule:
    # T√§glich neue Jobs erstellen (8 Uhr UTC = 9/10 Uhr Deutschland)
    - cron: '0 8 * * *'
    # Job Processing alle 30 Minuten
    - cron: '*/30 * * * *'
    # Zus√§tzlich: Sp√§tabends f√ºr s√ºdamerikanische Spiele
    - cron: '0 22 * * *'
  
  workflow_dispatch:
    inputs:
      force_collection:
        description: 'Force immediate data collection'
        required: false
        default: 'false'
      specific_league:
        description: 'League ID for specific collection'
        required: false
        default: ''

env:
  API_FOOTBALL_KEY: ${{ secrets.API_FOOTBALL_KEY }}
  ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
  DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
  # Enhanced Features Configuration
  ENABLE_ENHANCED_MAPPING: 'true'
  MAPPING_CONFIDENCE_THRESHOLD: '0.7'
  DATABASE_ANALYTICS: 'enabled'
  AUTO_LEARNING: 'true'

jobs:
  create-jobs:
    if: contains(github.event.schedule, '0 8 * * *') || contains(github.event.schedule, '0 22 * * *') || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Dependencies
      run: |
        pip install requests python-dateutil pytz
        
    - name: Initialize Enhanced Database
      run: |
        mkdir -p data
        # Use existing database schema instead of inline schema
        sqlite3 data/football_data.db < database_schema.sql
        
        # Initialize enhanced mapping tables
        python << 'EOF'
        import sys
        sys.path.append('.')
        try:
            from enhanced_mapping import EnhancedTeamMapper
            # Initialize enhanced mapper which creates mapping tables
            mapper = EnhancedTeamMapper(db_path='data/football_data.db', learn_mappings=True)
            print("‚úÖ Enhanced mapping database initialized")
        except Exception as e:
            print(f"‚ö†Ô∏è Enhanced mapping initialization failed: {e}")
            
        # Verify database setup
        import sqlite3
        conn = sqlite3.connect('data/football_data.db')
        cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [row[0] for row in cursor.fetchall()]
        print(f"‚úÖ Database initialized with {len(tables)} tables: {', '.join(tables)}")
        conn.close()
        EOF
    
    - name: Create Job Queue
      run: |
        python << 'EOF'
        import json
        import requests
        from datetime import datetime, timedelta
        import os
        import pytz
        
        # Liga-Konfiguration
        EUROPEAN_LEAGUES = {
            39: {'name': 'Premier League', 'priority': 'high', 'timezone': 'Europe/London'},
            140: {'name': 'La Liga', 'priority': 'high', 'timezone': 'Europe/Madrid'},
            135: {'name': 'Serie A', 'priority': 'high', 'timezone': 'Europe/Rome'},
            78: {'name': 'Bundesliga', 'priority': 'high', 'timezone': 'Europe/Berlin'},
            61: {'name': 'Ligue 1', 'priority': 'high', 'timezone': 'Europe/Paris'},
            2: {'name': 'Champions League', 'priority': 'highest', 'timezone': 'Europe/Zurich'},
            3: {'name': 'Europa League', 'priority': 'medium', 'timezone': 'Europe/Zurich'}
        }
        
        SOUTH_AMERICAN_LEAGUES = {
            13: {'name': 'Copa Libertadores', 'priority': 'high', 'timezone': 'America/Sao_Paulo'},
            71: {'name': 'Brasileir√£o Serie A', 'priority': 'medium', 'timezone': 'America/Sao_Paulo'},
            128: {'name': 'Liga Profesional Argentina', 'priority': 'medium', 'timezone': 'America/Argentina/Buenos_Aires'},
            239: {'name': 'Copa Sudamericana', 'priority': 'medium', 'timezone': 'America/Sao_Paulo'}
        }
        
        current_hour = datetime.now().hour
        if 6 <= current_hour <= 18:
            active_leagues = EUROPEAN_LEAGUES
            print("üá™üá∫ Focusing on European leagues")
        else:
            active_leagues = {**EUROPEAN_LEAGUES, **SOUTH_AMERICAN_LEAGUES}
            print("üåé Including South American leagues")
        
        specific_league = "${{ github.event.inputs.specific_league }}"
        if specific_league:
            league_id = int(specific_league)
            active_leagues = {k: v for k, v in active_leagues.items() if k == league_id}
            print(f"üéØ Focusing on specific league: {league_id}")
        
        headers = {
            'x-rapidapi-key': os.getenv('API_FOOTBALL_KEY'),
            'x-rapidapi-host': 'v3.football.api-sports.io'
        }
        
        upcoming_games = []
        today = datetime.now().strftime('%Y-%m-%d')
        tomorrow = (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')
        day_after = (datetime.now() + timedelta(days=2)).strftime('%Y-%m-%d')
        
        print(f"üîç Scanning dates: {today}, {tomorrow}, {day_after}")
        
        for league_id, config in active_leagues.items():
            print(f"\nüìä Checking {config['name']}...")
            
            for date in [today, tomorrow, day_after]:
                url = "https://v3.football.api-sports.io/fixtures"
                params = {'league': league_id, 'date': date, 'season': 2025, 'status': 'NS'}
                
                try:
                    response = requests.get(url, headers=headers, params=params, timeout=15)
                    
                    if response.status_code == 429:
                        print(f"‚ö†Ô∏è Rate limited for {config['name']}")
                        continue
                    elif response.status_code != 200:
                        print(f"‚ö†Ô∏è API Error {response.status_code} for {config['name']}")
                        continue
                    
                    data = response.json()
                    fixtures = data.get('response', [])
                    
                    for fixture in fixtures:
                        try:
                            kickoff_str = fixture['fixture']['date']
                            kickoff_utc = datetime.strptime(kickoff_str, '%Y-%m-%dT%H:%M:%S+00:00')
                            kickoff_utc = kickoff_utc.replace(tzinfo=pytz.UTC)
                            
                            now = datetime.now(pytz.UTC)
                            if now < kickoff_utc < now + timedelta(hours=72):
                                game_info = {
                                    'fixture_id': fixture['fixture']['id'],
                                    'kickoff_utc': kickoff_utc.isoformat(),
                                    'home_team': fixture['teams']['home']['name'],
                                    'away_team': fixture['teams']['away']['name'],
                                    'home_team_id': fixture['teams']['home']['id'],
                                    'away_team_id': fixture['teams']['away']['id'],
                                    'league': config['name'],
                                    'league_id': league_id,
                                    'country': fixture['league']['country'],
                                    'venue': fixture['fixture']['venue']['name'] if fixture['fixture']['venue'] else 'TBD',
                                    'priority': config['priority'],
                                    'timezone': config['timezone']
                                }
                                upcoming_games.append(game_info)
                                print(f"  ‚öΩ {game_info['home_team']} vs {game_info['away_team']} at {kickoff_utc}")
                        
                        except Exception as e:
                            print(f"  ‚ö†Ô∏è Error processing fixture: {e}")
                            continue
                            
                except Exception as e:
                    print(f"  ‚ùå Request failed: {e}")
                    continue
        
        print(f"\nüìà Total games found: {len(upcoming_games)}")
        
        # Erstelle Collection-Jobs
        jobs = []
        for game in upcoming_games:
            kickoff = datetime.fromisoformat(game['kickoff_utc'].replace('Z', '+00:00'))
            
            if game['priority'] == 'highest':
                schedules = [
                    {'offset_hours': 48, 'type': 'early_odds'},
                    {'offset_hours': 12, 'type': 'pre_match'},
                    {'offset_hours': 3, 'type': 'team_news'},
                    {'offset_hours': 1, 'type': 'final_data'}
                ]
            elif game['priority'] == 'high':
                schedules = [
                    {'offset_hours': 24, 'type': 'early_odds'},
                    {'offset_hours': 3, 'type': 'team_news'},
                    {'offset_hours': 1, 'type': 'final_data'}
                ]
            else:
                schedules = [
                    {'offset_hours': 6, 'type': 'team_news'},
                    {'offset_hours': 1, 'type': 'final_data'}
                ]
            
            for schedule in schedules:
                collection_time = kickoff - timedelta(hours=schedule['offset_hours'])
                now = datetime.now(pytz.UTC)
                
                if collection_time > now:
                    job = {
                        'id': f"{game['fixture_id']}_{schedule['type']}",
                        'fixture_id': game['fixture_id'],
                        'scheduled_for': collection_time.isoformat(),
                        'status': 'pending',
                        'type': schedule['type'],
                        'priority': game['priority'],
                        'game_info': game,
                        'created_at': now.isoformat()
                    }
                    jobs.append(job)
        
        # Merge mit existierenden Jobs
        try:
            jobs_path = os.path.join(os.path.dirname(__file__), 'jobs.json')
            with open(jobs_path, 'r') as f:
                existing_jobs = json.load(f)
        except FileNotFoundError:
            existing_jobs = []
        
        # Cleanup alter Jobs
        now = datetime.now(pytz.UTC)
        active_jobs = []
        
        for job in existing_jobs:
            scheduled_time = datetime.fromisoformat(job['scheduled_for'].replace('Z', '+00:00'))
            
            if (job['status'] == 'pending' and scheduled_time > now - timedelta(hours=2)) or \
               (job['status'] in ['completed', 'failed'] and scheduled_time > now - timedelta(days=3)):
                active_jobs.append(job)
        
        existing_ids = {job['id'] for job in active_jobs}
        new_jobs = [job for job in jobs if job['id'] not in existing_ids]
        
        all_jobs = active_jobs + new_jobs
        all_jobs.sort(key=lambda x: x['scheduled_for'])
        
        jobs_path = os.path.join(os.path.dirname(__file__), 'jobs.json')
        with open(jobs_path, 'w') as f:
            json.dump(all_jobs, f)
        
        print(f"\nüìä Job Summary:")
        print(f"  üì• Existing: {len(active_jobs)}")
        print(f"  ‚ûï New: {len(new_jobs)}")
        print(f"  üìã Total: {len(all_jobs)}")
        
        # Discord Notification
        webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
        if webhook_url and new_jobs:
            embed = {
                "title": "üéØ New Games Scheduled",
                "description": f"Found {len(upcoming_games)} upcoming games\nCreated {len(new_jobs)} collection jobs",
                "color": 0x00ff00,
                "fields": [
                    {
                        "name": "Next Collections",
                        "value": "\n".join([f"‚öΩ {job['game_info']['home_team']} vs {job['game_info']['away_team']}" for job in new_jobs[:5]]),
                        "inline": False
                    }
                ],
                "timestamp": datetime.now().isoformat()
            }
            
            try:
                requests.post(webhook_url, json={"embeds": [embed]})
                print("‚úÖ Discord notification sent")
            except:
                print("‚ö†Ô∏è Discord notification failed")
        EOF
        
    - name: Commit Jobs
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        git commit -m "üîÑ Update job queue and database" || exit 0
        git pull --rebase
        git push

  process-jobs:
    if: contains(github.event.schedule, '*/59 * * * *') || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Dependencies
      run: |
        pip install requests python-dateutil pytz

    - name: Process Pending Jobs with Logging
      run: |
        python << 'EOF'
        import json
        import requests
        from datetime import datetime, timedelta
        import os
        import pytz
        import sqlite3
        import logging

        # Logging-Konfiguration
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("pipeline")

        DATA_DIR = 'data'
        os.makedirs(DATA_DIR, exist_ok=True)

        def write_json_with_logging(filename, data, job, reason=""):
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                logger.info(f"‚úÖ Datei geschrieben: {filename}")
                job['file_write_status'] = 'success'
            except Exception as e:
                logger.error(f"‚ùå Fehler beim Schreiben der Datei {filename}: {e}")
                job['file_write_status'] = 'error'
                job['error_reason'] = f"File-Write-Error: {e}" + (f" ({reason})" if reason else "")

        def collect_and_store_odds(job):
            fixture_id = job.get('fixture_id', job.get('id'))
            odds_api_key = os.getenv('ODDS_API_KEY', '')
            odds_url = f"https://api.the-odds-api.com/v4/sports/{fixture_id}/odds/"
            odds_params = {'apiKey': odds_api_key, 'regions': 'eu', 'markets': 'h2h'}
            try:
                response = requests.get(odds_url, params=odds_params, timeout=15)
                if response.status_code == 200:
                    odds_data = response.json()
                    if not odds_data:
                        logger.warning(f"‚ö†Ô∏è Odds-API liefert keine Daten f√ºr Fixture {fixture_id}")
                        job['error_reason'] = "Odds-API returned empty data"
                    else:
                        filename = os.path.join(DATA_DIR, f"early_odds_{fixture_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                        write_json_with_logging(filename, odds_data, job)
                else:
                    logger.error(f"‚ùå Odds-API Error {response.status_code} f√ºr Fixture {fixture_id}: {response.text}")
                    job['error_reason'] = f"Odds-API Error {response.status_code}: {response.text}"
            except Exception as e:
                logger.error(f"‚ùå Odds-API Exception f√ºr Fixture {fixture_id}: {e}")
                job['error_reason'] = f"Odds-API Exception: {e}"

        def collect_and_store_final_data(job):
            fixture_id = job.get('fixture_id', job.get('id'))
            try:
                final_data = {"dummy": "final_data"}
                filename = os.path.join(DATA_DIR, f"final_data_{fixture_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                write_json_with_logging(filename, final_data, job)
            except Exception as e:
                logger.error(f"‚ùå Fehler beim Finaldaten-API-Call f√ºr Fixture {fixture_id}: {e}")
                job['error_reason'] = f"Final-Data-API Exception: {e}"

            jobs_path = os.path.join(os.path.dirname(__file__), 'jobs.json')
            with open(jobs_path, 'r', encoding='utf-8') as f:
                jobs = json.load(f)

            for job in jobs:
                if job.get('status') == 'pending':
                    logger.info(f"üîÑ Starte Verarbeitung f√ºr Fixture {job.get('fixture_id', job.get('id'))}: {job.get('home_team')} vs {job.get('away_team')}")
                    collect_and_store_odds(job)
                    collect_and_store_final_data(job)
                    if job.get('file_write_status') == 'success':
                        job['status'] = 'completed'
                else:
                    job['status'] = 'failed'

            with open(jobs_path, 'w', encoding='utf-8') as f:
                json.dump(jobs, f, indent=2, ensure_ascii=False)

            logger.info("‚úÖ Job-Verarbeitung abgeschlossen. Siehe jobs.json f√ºr Status und Fehler.")
           
            job['status'] = 'completed' if success else 'failed'
            job['processed_at'] = now.isoformat()
            if not success:
                job['error_reason'] = 'Collection failed'
            
                processed_jobs.append(job['id'])
                processed_count += 1
            
            elif now > scheduled_time + timedelta(minutes=45):
                # Job zu sp√§t
                job['status'] = 'expired'
                job['processed_at'] = now.isoformat()
                job['error_reason'] = 'Expired (too late)'
                expired_jobs.append(job['id'])
            
            # Aktualisiere jobs.json
            if processed_jobs or expired_jobs:
                jobs_path = os.path.join(os.path.dirname(__file__), 'jobs.json')
                with open(jobs_path, 'w') as f:
                    json.dump(jobs, f)
                
                print(f"\nüìä Processing Summary:")
                print(f"‚úÖ Completed: {len(processed_jobs)}")
                print(f"‚è∞ Expired: {len(expired_jobs)}")
                
                # Discord Success Notification
                webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
                if webhook_url and processed_jobs:
                    embed = {
                        "title": "‚úÖ Data Collection Complete",
                        "description": f"Successfully processed {len(processed_jobs)} jobs",
                        "fields": [
                            {
                                "name": "Jobs Completed",
                                "value": "\n".join(processed_jobs[:5]),
                                "inline": True
                            }
                        ],
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    try:
                        requests.post(webhook_url, json={"embeds": [embed]})
                    except:
                        pass
            else:
                print("‚ÑπÔ∏è No jobs ready for processing")
            EOF
      env:
        API_FOOTBALL_KEY: ${{ secrets.API_FOOTBALL_KEY }}
        ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        
    - name: Commit Results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        git commit -m "üìä Process data collection jobs" || exit 0
        git pull --rebase
        git push

  health-check:
    runs-on: ubuntu-latest
    needs: [process-jobs]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Pipeline Health Check
      run: |
        python << 'EOF'
        import json
        import os
        from datetime import datetime, timedelta
        
        health_status = {
            'timestamp': datetime.now().isoformat(),
            'pipeline_status': 'healthy',
            'issues': [],
            'recommendations': []
        }
        
        # Check jobs.json
        try:
            jobs_path = os.path.join(os.path.dirname(__file__), 'jobs.json')
            with open(jobs_path, 'r') as f:
                jobs = json.load(f)
            
            pending_jobs = [j for j in jobs if j['status'] == 'pending']
            failed_jobs = [j for j in jobs if j['status'] == 'failed']
            expired_jobs = [j for j in jobs if j['status'] == 'expired']
            completed_jobs = [j for j in jobs if j['status'] == 'completed']
            
            # Health indicators
            if len(failed_jobs) > len(completed_jobs) * 0.2:  # >20% failure rate
                health_status['issues'].append('High job failure rate')
                health_status['pipeline_status'] = 'degraded'
            
            if len(expired_jobs) > 10:
                health_status['issues'].append('Many expired jobs - possible scheduling issues')
                health_status['recommendations'].append('Check API rate limits and processing capacity')
            
            if len(pending_jobs) > 50:
                health_status['issues'].append('Large job backlog')
                health_status['recommendations'].append('Consider increasing processing frequency')
            
            # Check recent activity (last 24h)
            yesterday = datetime.now() - timedelta(hours=24)
            recent_jobs = [j for j in jobs if j.get('processed_at', '1970-01-01') >= yesterday.isoformat()]
            
            if len(recent_jobs) == 0:
                health_status['issues'].append('No recent job activity')
                health_status['pipeline_status'] = 'unhealthy'
            
        except FileNotFoundError:
            health_status['issues'].append('jobs.json not found')
            health_status['pipeline_status'] = 'unhealthy'
        except Exception as e:
            health_status['issues'].append(f'Job analysis failed: {e}')
        
        # Check database
        if os.path.exists('data/football_data.db'):
            try:
                import sqlite3
                conn = sqlite3.connect('data/football_data.db')
                
                # Check recent data
                cursor = conn.execute("SELECT COUNT(*) FROM odds_history WHERE collected_at >= datetime('now', '-24 hours')")
                recent_odds = cursor.fetchone()[0]
                
                cursor = conn.execute("SELECT COUNT(*) FROM fixtures WHERE kickoff_utc > datetime('now')")
                future_fixtures = cursor.fetchone()[0]
                
                if recent_odds == 0:
                    health_status['issues'].append('No recent odds data collected')
                
                if future_fixtures == 0:
                    health_status['issues'].append('No upcoming fixtures found')
                
                conn.close()
                
            except Exception as e:
                health_status['issues'].append(f'Database check failed: {e}')
        else:
            health_status['issues'].append('Database not found')
            health_status['pipeline_status'] = 'unhealthy'
        
        # Set overall status
        if len(health_status['issues']) > 3:
            health_status['pipeline_status'] = 'unhealthy'
        elif len(health_status['issues']) > 0:
            health_status['pipeline_status'] = 'degraded'
        
        # Save health report
        with open('data/health_status.json', 'w') as f:
            json.dump(health_status, f, indent=2)
        
        print(f"üè• Pipeline Health Check Complete")
        print(f"   Status: {health_status['pipeline_status'].upper()}")
        print(f"   Issues: {len(health_status['issues'])}")
        
        for issue in health_status['issues']:
            print(f"   ‚ö†Ô∏è {issue}")
        
        # Discord notification f√ºr unhealthy status
        if health_status['pipeline_status'] == 'unhealthy':
            webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
            if webhook_url:
                embed = {
                    "title": "üö® Pipeline Health Alert",
                    "description": f"Pipeline status: **{health_status['pipeline_status'].upper()}**",
                    "color": 0xff0000,
                    "fields": [
                        {
                            "name": "Issues Detected",
                            "value": "\n".join([f"‚Ä¢ {issue}" for issue in health_status['issues'][:5]]),
                            "inline": False
                        }
                    ],
                    "timestamp": health_status['timestamp']
                }
                
                if health_status['recommendations']:
                    embed['fields'].append({
                        "name": "Recommendations",
                        "value": "\n".join([f"‚Ä¢ {rec}" for rec in health_status['recommendations'][:3]]),
                        "inline": False
                    })
                
                try:
                    import requests
                    requests.post(webhook_url, json={"embeds": [embed]})
                except:
                    pass
        
        # Exit with error code if unhealthy
        if health_status['pipeline_status'] == 'unhealthy':
            exit(1)
        EOF
      env:
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        
    - name: Commit Health Status
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/health_status.json
        git commit -m "üè• Update pipeline health status" || exit 0
        git pull --rebase
        git push
      env:
        API_FOOTBALL_KEY: ${{ secrets.API_FOOTBALL_KEY }}
        ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        
    - name: Commit Results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        git commit -m "üìä Process data collection jobs" || exit 0
        git pull --rebase
        git push

  daily-maintenance:
    if: contains(github.event.schedule, '0 8 * * *')
    runs-on: ubuntu-latest
    needs: [create-jobs]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Database Maintenance
      run: |
        python << 'EOF'
        import sqlite3
        import os
        from datetime import datetime, timedelta
        
        if os.path.exists('data/football_data.db'):
            conn = sqlite3.connect('data/football_data.db')
            
            # Cleanup alte Daten (√§lter als 6 Monate)
            cutoff_date = datetime.now() - timedelta(days=180)
            
            # Bereinige odds_history
            cursor = conn.execute("""
                DELETE FROM odds_history 
                WHERE collected_at < ?
            """, (cutoff_date,))
            deleted_odds = cursor.rowcount
            
            # Bereinige team_events
            cursor = conn.execute("""
                DELETE FROM team_events 
                WHERE detected_at < ? AND end_date < ?
            """, (cutoff_date, datetime.now().date()))
            deleted_events = cursor.rowcount
            
            # Vacuum f√ºr bessere Performance
            conn.commit()
            conn.execute("VACUUM")
            
            # Statistiken
            cursor = conn.execute("SELECT COUNT(*) FROM odds_history")
            odds_count = cursor.fetchone()[0]
            
            cursor = conn.execute("SELECT COUNT(*) FROM fixtures")
            fixtures_count = cursor.fetchone()[0]
            
            cursor = conn.execute("SELECT COUNT(*) FROM teams")
            teams_count = cursor.fetchone()[0]
            
            conn.commit()
            conn.close()
            
            print(f"üßπ Database maintenance completed")
            print(f"üóëÔ∏è Deleted: {deleted_odds} old odds, {deleted_events} old events")
            print(f"üìä Current: {odds_count} odds, {fixtures_count} fixtures, {teams_count} teams")
        else:
            print("‚ö†Ô∏è Database not found")
        EOF
    
    - name: Generate Weekly Report
      run: |
        python << 'EOF'
        import json
        import sqlite3
        import os
        from datetime import datetime, timedelta
        
        if os.path.exists('data/football_data.db'):
            conn = sqlite3.connect('data/football_data.db')
            conn.row_factory = sqlite3.Row
            
            # Weekly Report
            week_ago = datetime.now() - timedelta(days=7)
            
            # Odds collected last week
            cursor = conn.execute("""
                SELECT COUNT(*) as count FROM odds_history 
                WHERE collected_at >= ?
            """, (week_ago,))
            weekly_odds = cursor.fetchone()[0]
            
            # Games with data
            cursor = conn.execute("""
                SELECT COUNT(DISTINCT fixture_id) as count FROM odds_history 
                WHERE collected_at >= ?
            """, (week_ago,))
            weekly_games = cursor.fetchone()[0]
            
            # Top leagues by activity
            cursor = conn.execute("""
                SELECT l.name, COUNT(DISTINCT oh.fixture_id) as games
                FROM odds_history oh
                JOIN fixtures f ON oh.fixture_id = f.id
                JOIN leagues l ON f.league_id = l.id
                WHERE oh.collected_at >= ?
                GROUP BY l.id, l.name
                ORDER BY games DESC
                LIMIT 5
            """, (week_ago,))
            top_leagues = cursor.fetchall()
            
            # Bookmaker distribution
            cursor = conn.execute("""
                SELECT bookmaker, COUNT(*) as records
                FROM odds_history
                WHERE collected_at >= ?
                GROUP BY bookmaker
                ORDER BY records DESC
                LIMIT 10
            """, (week_ago,))
            bookmakers = cursor.fetchall()
            
            # Collection phases
            cursor = conn.execute("""
                SELECT collection_phase, COUNT(*) as records
                FROM odds_history
                WHERE collected_at >= ?
                GROUP BY collection_phase
                ORDER BY records DESC
            """, (week_ago,))
            phases = cursor.fetchall()
            
            report = {
                'report_date': datetime.now().isoformat(),
                'period': 'last_7_days',
                'summary': {
                    'odds_collected': weekly_odds,
                    'games_tracked': weekly_games,
                    'top_leagues': [{'name': row[0], 'games': row[1]} for row in top_leagues],
                    'bookmakers': [{'name': row[0], 'records': row[1]} for row in bookmakers],
                    'collection_phases': [{'phase': row[0], 'records': row[1]} for row in phases]
                }
            }
            
            with open('data/weekly_report.json', 'w') as f:
                json.dump(report, f, indent=2)
            
            print(f"üìã Weekly Report Generated:")
            print(f"   üìä {weekly_odds} odds records collected")
            print(f"   ‚öΩ {weekly_games} games tracked")
            if top_leagues:
                print(f"   üèÜ Top league: {top_leagues[0][0]} ({top_leagues[0][1]} games)")
            
            # Discord Weekly Report
            webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
            if webhook_url:
                embed = {
                    "title": "üìã Weekly Football Data Report",
                    "description": f"Data collection summary for the last 7 days",
                    "color": 0x0099ff,
                    "fields": [
                        {
                            "name": "üìä Data Collected",
                            "value": f"**{weekly_odds:,}** odds records\n**{weekly_games}** games tracked",
                            "inline": True
                        },
                        {
                            "name": "üèÜ Top Leagues",
                            "value": "\n".join([f"‚Ä¢ {league['name']}: {league['games']} games" for league in report['summary']['top_leagues'][:3]]),
                            "inline": True
                        },
                        {
                            "name": "üìà Collection Phases",
                            "value": "\n".join([f"‚Ä¢ {phase['phase']}: {phase['records']}" for phase in report['summary']['collection_phases']]),
                            "inline": False
                        }
                    ],
                    "timestamp": datetime.now().isoformat(),
                    "footer": {
                        "text": "Football Data Pipeline ‚Ä¢ Weekly Report"
                    }
                }
                
                try:
                    requests.post(webhook_url, json={"embeds": [embed]})
                    print("‚úÖ Weekly report sent to Discord")
                except Exception as e:
                    print(f"‚ö†Ô∏è Discord report failed: {e}")
            
            conn.close()
        else:
            print("‚ö†Ô∏è Database not found for reporting")
        EOF
      env:
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}

    - name: Generate Dashboard Data
      run: |
        python << 'EOF'
        import sqlite3
        import json
        import os
        from datetime import datetime, timedelta
        
        if os.path.exists('data/football_data.db'):
            conn = sqlite3.connect('data/football_data.db')
            conn.row_factory = sqlite3.Row
            
            dashboard_data = {}
            
            # Upcoming games with latest odds
            upcoming = conn.execute("""
                SELECT f.id, f.kickoff_utc, ht.name as home_team, at.name as away_team,
                       l.name as league, l.country,
                       oh.home_odds, oh.draw_odds, oh.away_odds, oh.bookmaker,
                       oh.collected_at
                FROM fixtures f
                JOIN teams ht ON f.home_team_id = ht.id
                JOIN teams at ON f.away_team_id = at.id  
                JOIN leagues l ON f.league_id = l.id
                LEFT JOIN (
                    SELECT fixture_id, home_odds, draw_odds, away_odds, bookmaker, collected_at,
                           ROW_NUMBER() OVER (PARTITION BY fixture_id ORDER BY collected_at DESC) as rn
                    FROM odds_history WHERE market_type = 'h2h'
                ) oh ON f.id = oh.fixture_id AND oh.rn = 1
                WHERE f.kickoff_utc > datetime('now')
                AND f.kickoff_utc < datetime('now', '+48 hours')
                ORDER BY f.kickoff_utc
                LIMIT 20
            """).fetchall()
            
            dashboard_data['upcoming_games'] = [dict(row) for row in upcoming]
            
            # Recent odds movements
            movements = conn.execute("""
                WITH recent_odds AS (
                    SELECT fixture_id, home_odds, away_odds, collected_at, bookmaker,
                           LAG(home_odds) OVER (PARTITION BY fixture_id, bookmaker ORDER BY collected_at) as prev_home,
                           LAG(away_odds) OVER (PARTITION BY fixture_id, bookmaker ORDER BY collected_at) as prev_away
                    FROM odds_history 
                    WHERE collected_at >= datetime('now', '-24 hours')
                    AND market_type = 'h2h'
                )
                SELECT r.*, f.kickoff_utc, ht.name as home_team, at.name as away_team
                FROM recent_odds r
                JOIN fixtures f ON r.fixture_id = f.id
                JOIN teams ht ON f.home_team_id = ht.id
                JOIN teams at ON f.away_team_id = at.id
                WHERE prev_home IS NOT NULL
                AND (ABS(home_odds - prev_home) / prev_home > 0.1 
                     OR ABS(away_odds - prev_away) / prev_away > 0.1)
                ORDER BY r.collected_at DESC
                LIMIT 10
            """).fetchall()
            
            dashboard_data['odds_movements'] = [dict(row) for row in movements]
            
            # League statistics
            league_stats = conn.execute("""
                SELECT l.name, l.country, COUNT(DISTINCT f.id) as total_games,
                       COUNT(DISTINCT oh.id) as odds_records
                FROM leagues l
                LEFT JOIN fixtures f ON l.id = f.league_id  
                LEFT JOIN odds_history oh ON f.id = oh.fixture_id
                WHERE f.kickoff_utc >= datetime('now', '-30 days')
                GROUP BY l.id, l.name, l.country
                ORDER BY total_games DESC
            """).fetchall()
            
            dashboard_data['league_stats'] = [dict(row) for row in league_stats]
            
            # Team performance
            team_performance = conn.execute("""
                SELECT t.name, ts.win_percentage, ts.goals_for, ts.goals_against,
                       ts.matches_played, l.name as league
                FROM team_statistics ts
                JOIN teams t ON ts.team_id = t.id
                JOIN leagues l ON ts.league_id = l.id
                WHERE ts.collection_date >= date('now', '-7 days')
                AND ts.matches_played >= 5
                ORDER BY ts.win_percentage DESC
                LIMIT 20
            """).fetchall()
            
            dashboard_data['top_teams'] = [dict(row) for row in team_performance]
            
            # Meta information
            dashboard_data['last_updated'] = datetime.now().isoformat()
            dashboard_data['stats'] = {
                'total_fixtures': conn.execute("SELECT COUNT(*) FROM fixtures").fetchone()[0],
                'total_odds_records': conn.execute("SELECT COUNT(*) FROM odds_history").fetchone()[0],
                'active_leagues': len(dashboard_data['league_stats'])
            }
            
            # Save dashboard data
            with open('data/dashboard_data.json', 'w') as f:
                json.dump(dashboard_data, f, indent=2)
            
            print(f"üìä Dashboard data generated with {len(dashboard_data['upcoming_games'])} games")
            conn.close()
        else:
            print("‚ö†Ô∏è Database not found for dashboard")
        EOF

    - name: API Usage Analytics
      run: |
        python << 'EOF'
        import json
        import sqlite3
        import os
        from datetime import datetime, timedelta
        
        if os.path.exists('data/football_data.db'):
            conn = sqlite3.connect('data/football_data.db')
            conn.row_factory = sqlite3.Row
            
            # Berechne API-Usage f√ºr die letzten 24h
            yesterday = datetime.now() - timedelta(days=1)
            
            # Anzahl der API-Calls sch√§tzen basierend auf Jobs
            try:
                jobs_path = os.path.join(os.path.dirname(__file__), 'jobs.json')
                with open(jobs_path, 'r') as f:
                    jobs = json.load(f)
                
                completed_jobs = [job for job in jobs if job['status'] == 'completed' and 
                                job['processed_at'] >= yesterday.isoformat()]
                
                # Sch√§tze API-Calls pro Job-Typ
                api_usage = {
                    'early_odds': 2,     # Fixture details + Odds API
                    'pre_match': 4,      # + Team stats + H2H
                    'team_news': 1,      # Nur Odds API
                    'final_data': 2      # Lineups + Odds API
                }
                
                total_api_calls = sum(api_usage.get(job['type'], 1) for job in completed_jobs)
                
                # API-Football Calls (max 100/Tag im Free Plan)
                api_football_calls = sum(
                    3 if job['type'] in ['pre_match', 'final_data'] else 1 
                    for job in completed_jobs
                )
                
                # Odds API Calls (max 500/Monat im Free Plan)
                odds_api_calls = len(completed_jobs)  # 1 Call pro Job
                
                usage_report = {
                    'date': datetime.now().date().isoformat(),
                    'completed_jobs': len(completed_jobs),
                    'estimated_total_calls': total_api_calls,
                    'api_football_calls': api_football_calls,
                    'odds_api_calls': odds_api_calls,
                    'api_football_remaining': max(0, 100 - api_football_calls),
                    'usage_status': 'green' if api_football_calls < 80 else 'yellow' if api_football_calls < 95 else 'red'
                }
                
                print(f"üìä API Usage Report:")
                print(f"   üìû Total API Calls: {total_api_calls}")
                print(f"   üèà API-Football: {api_football_calls}/100 ({usage_report['api_football_remaining']} remaining)")
                print(f"   üí∞ Odds API: {odds_api_calls}/500")
                print(f"   ‚úÖ Completed Jobs: {len(completed_jobs)}")
                
                with open('data/api_usage.json', 'w') as f:
                    json.dump(usage_report, f, indent=2)
                
                # Warning bei hoher API-Usage
                if usage_report['usage_status'] in ['yellow', 'red']:
                    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
                    if webhook_url:
                        color = 0xffaa00 if usage_report['usage_status'] == 'yellow' else 0xff0000
                        embed = {
                            "title": "‚ö†Ô∏è High API Usage Warning",
                            "description": f"API-Football usage: {api_football_calls}/100 daily limit",
                            "color": color,
                            "fields": [
                                {
                                    "name": "Current Usage",
                                    "value": f"**API-Football**: {api_football_calls}/100\n**Odds API**: {odds_api_calls}/500",
                                    "inline": True
                                },
                                {
                                    "name": "Recommendation", 
                                    "value": "Consider reducing league coverage or collection frequency",
                                    "inline": True
                                }
                            ],
                            "timestamp": datetime.now().isoformat()
                        }
                        
                        try:
                            import requests
                            requests.post(webhook_url, json={"embeds": [embed]})
                        except:
                            pass
                            
            except Exception as e:
                print(f"‚ö†Ô∏è API usage calculation failed: {e}")
        
        else:
            print("‚ö†Ô∏è Database not found for API usage analytics")
        EOF
      env:
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}

    - name: Generate Enhanced Daily Mapping Report
      run: |
        python << 'EOF'
        import json
        import os
        from datetime import datetime

        try:
            import sys
            sys.path.append('.')
            from enhanced_mapping import EnhancedTeamMapper
            
            if os.path.exists('data/football_data.db'):
                mapper = EnhancedTeamMapper(db_path='data/football_data.db')
                
                # Get current mapping statistics  
                stats = mapper.stats
                report = mapper.get_mapping_report(days=1) if stats.total_attempts >= 3 else None
                
                if stats.total_attempts > 0:
                    print(f"üìä Enhanced Daily Mapping Report:")
                    print(f"   üéØ Success Rate: {stats.success_rate:.1%}")
                    print(f"   üìà Total Attempts: {stats.total_attempts}")
                    print(f"   ‚ö° Avg Confidence: {stats.avg_confidence:.3f}")
                    
                    # Save the statistics
                    stats_data = {
                        'date': datetime.now().date().isoformat(),
                        'stats': {
                            'total_attempts': stats.total_attempts,
                            'successful_mappings': stats.successful_mappings,
                            'failed_mappings': stats.failed_mappings,
                            'success_rate': stats.success_rate,
                            'avg_confidence': stats.avg_confidence,
                            'strategy_usage': stats.strategy_usage
                        },
                        'report': report
                    }
                    
                    with open('data/daily_mapping_stats.json', 'w') as f:
                        json.dump(stats_data, f, indent=2)
                    
                    # Enhanced Discord notification with better formatting
                    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
                    if webhook_url and stats.total_attempts >= 5:  # Only for significant activity
                        embed_color = 0x00ff00 if stats.success_rate > 0.85 else 0xffaa00 if stats.success_rate > 0.65 else 0xff0000
                        
                        embed = {
                            "title": "üéØ Enhanced Team Mapping Report",
                            "description": f"Intelligent mapping performance for {datetime.now().strftime('%Y-%m-%d')}",
                            "color": embed_color,
                            "fields": [
                                {
                                    "name": "üìä Performance Metrics",
                                    "value": f"**Success Rate**: {stats.success_rate:.1%}\n**Attempts**: {stats.total_attempts}\n**Avg Confidence**: {stats.avg_confidence:.3f}",
                                    "inline": True
                                }
                            ],
                            "timestamp": datetime.now().isoformat()
                        }
                        
                        # Add learning progress if available
                        if report:
                            learned_count = report.get('learned_mappings_count', 0)
                            manual_count = report.get('manual_mappings_count', 0)
                            embed['fields'].append({
                                "name": "üß† Knowledge Base",
                                "value": f"**Learned**: {learned_count}\n**Manual**: {manual_count}\n**Total**: {learned_count + manual_count}",
                                "inline": True
                            })
                            
                            # Add strategy performance
                            if report.get('strategy_performance'):
                                top_strategies = sorted(report['strategy_performance'], 
                                                      key=lambda x: x['success_rate'], reverse=True)[:3]
                                strategy_text = "\n".join([
                                    f"‚Ä¢ {strategy['strategy_used']}: {strategy['success_rate']:.1%}"
                                    for strategy in top_strategies
                                ])
                                embed['fields'].append({
                                    "name": "üèÜ Top Strategies",
                                    "value": strategy_text,
                                    "inline": False
                                })
                        
                        # Add improvement suggestions for poor performance
                        if stats.success_rate < 0.65:
                            suggestions = []
                            if stats.failed_mappings > stats.successful_mappings * 0.5:
                                suggestions.append("‚Ä¢ High failure rate - manual verification needed")
                            if stats.avg_confidence < 0.5:
                                suggestions.append("‚Ä¢ Low confidence - expand mapping database")
                            
                            if suggestions:
                                embed['fields'].append({
                                    "name": "üí° Improvement Suggestions",
                                    "value": "\n".join(suggestions),
                                    "inline": False
                                })
                        
                        try:
                            import requests
                            requests.post(webhook_url, json={"embeds": [embed]})
                            print("‚úÖ Enhanced mapping report sent to Discord")
                        except Exception as e:
                            print(f"‚ö†Ô∏è Discord mapping report failed: {e}")
                
                else:
                    print("‚ÑπÔ∏è No mapping attempts today - report skipped")
            else:
                print("‚ö†Ô∏è Database not found for mapping report")
                
        except ImportError:
            print("‚ö†Ô∏è Enhanced mapping not available - using basic reporting")
        except Exception as e:
            print(f"‚ö†Ô∏è Enhanced mapping report generation failed: {e}")
            import traceback
            traceback.print_exc()
        EOF
      env:
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}

    - name: Commit Reports
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/
        git commit -m "üìã Generate weekly reports and dashboard data" || exit 0
        git pull --rebase
        git push

  error-notification:
    if: failure()
    needs: [create-jobs, process-jobs, daily-maintenance]
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4

    - name: Notify Discord on Failure
      run: |
        webhook_url="${{ secrets.DISCORD_WEBHOOK_URL }}"
        if [ ! -z "$webhook_url" ]; then
          curl -H "Content-Type: application/json" -X POST -d '{
            "embeds": [{
              "title": "‚ùå Pipeline Error",
              "description": "Football data collection pipeline encountered an error",
              "color": 16711680,
              "fields": [
                {
                  "name": "Repository",
                  "value": "${{ github.repository }}",
                  "inline": true
                },
                {
                  "name": "Workflow",
                  "value": "${{ github.workflow }}",
                  "inline": true
                },
                {
                  "name": "Run ID",
                  "value": "[#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})",
                  "inline": true
                }
              ],
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%S.000Z)'"
            }]
          }' "$webhook_url"
        fi
